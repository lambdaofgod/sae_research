#+title: Experiment Log

* SAE notebook - setup
:PROPERTIES:
:CREATED:  <2025-10-08 Wed> [01:43]
:END:

Niestety wygląda na to że na SAE Lens/transformer lens ma problemy...
Na RTX model poprawnie ma średnią L0 ~ 301 ale na MPS jest to z 1.5k



* Thresholding pursuit
:PROPERTIES:
:CREATED:  <2025-10-11 Sat> [18:34]
:END:

Odpaliłem coś takiego (acts=Y jest $B \times L \times H$, $W_{dec}$ jest $D \times H$ - rozmiar słownika razy rozmiar warstwy ukrytej)

#+BEGIN_SRC python :session experiment_log.org  :exports both
def sparse_lstq(A, Y, support):
    """
    find X
    A[:,support]X = Y
    """
    A_in = A[support].float().T
    Y_in = Y.float().T
    coefficients = torch.linalg.lstsq(A_in, Y_in).solution.T
    x = torch.zeros(Y.shape[0], A.shape[0]).to(Y.device)
    x[:,support] = coefficients
    return x

def instance_thresholding_support(W_dec, y, topk):
    """
    get support like in thresholding algorithm and then fit linear regression on this support
    """
    return torch.topk((y @ W_dec.T).abs(), topk, dim=-1).indices.flatten().unique()


def instance_thresholding_pursuit(acts, W_dec, topk):
    x = torch.zeros(*acts.shape[:2], W_dec.shape[0]).to(acts.device).half()
    for i in range(acts.shape[0]):
        Y = acts[i]
        support = instance_thresholding_support(W_dec, acts, topk)
        coeffs = sparse_lstq(W_dec, Y, support)

        x[i] = coeffs
    return x
#+END_SRC

W skrócie biorę nośnik topk per

** Ciekawe obserwacje
:PROPERTIES:
:CREATED:  <2025-10-11 Sat> [18:39]
:END:

To jest z SAE

#+BEGIN_SRC python :session experiment_log.org  :exports both
sae_config = dict(

    release="gemma-scope-2b-pt-res",  # see other options in sae_lens/pretrained_saes.yaml
    sae_id="layer_19/width_16k/average_l0_279",
)
#+END_SRC


#+BEGIN_QUOTE
Orig 3.2734375
###
sae nonzero
l0 per token: 315.0157470703125
l0 per instance: 6891.25
reconstr_sae 3.73046875
###
iht nonzero
l0 per token: 86.93897247314453
l0 per instance: 3329.5
reconstr_iht 3.9453125
###
instance iht nonzero
l0 per token: 55.86023712158203
l0 per instance: 2643.75
reconstr_instance_iht 3.931640625
###
pursuit nonzero
l0 per token: 280.2775573730469
l0 per instance: 530.0
reconstr_instance_thresholding_pursuit 3.953125
###
Zero 12.453125
#+END_QUOTE

Czyli wygląda jakby dla tego modelu który ma dosyć wysokie l0 nasz model miał per-token podobne l0
ale za to per cały tekst u nas jest znacznie lepiej

Co zabawniej to jak l0 SAE jest niższe to ten instance pursuit ma lepszą stratę, oczywiście wyższe l0 per token
ale i tak dalej per instancja l0 jest mniejsze



* SAEBench
:PROPERTIES:
:CREATED:  <2025-10-13 Mon> [22:55]
:END:

Ten notebook w repo można wykorzystać z customowymi SAE

Notebook odpala ich ewaluację i można go użyć do porównania paru modeli

** SAEBench - integracja
:PROPERTIES:
:CREATED:  <2025-10-15 Wed> [00:09]
:END:

Dodałem InstanceHardThresholdingSAE (nazwa może nie do końca fortunna)
i wyciągnąłem moduły do dopasowywania modelu liniowego i thresholdingu

** Metryki

SAEBench ma swietna strone na Neuronpedii

https://www.neuronpedia.org/sae-bench/info

** SAEBench demo

Odpalilem MPSAE i IHTPSAE w SAEBench - dzialaja i jedyny problem jest na koncu ze sparse probe (ale ten blad juz w sumie wychodzil tez bez tych SAE)

** DVC
:PROPERTIES:
:CREATED:  <2025-10-18 sob> [13:46]
:END:



* Negative Results for SAEs On Downstream Tasks and Deprioritising SAE Research (GDM Mech Interp Team Progress Update #2)
:PROPERTIES:
:CREATED:  <2025-10-20 pon> [23:26]
:END:

https://www.alignmentforum.org/posts/4uXCAJNuPKtKBsi28/sae-progress-update-2-draft
W tym poście jest sygnalizowane że SAE wyglądają na ślepą uliczkę, ALE w zasadzie to autorzy sami wymienili potencjalnie przydatne kierunki:

#+BEGIN_QUOTE
There are also other valuable projects, for example, *are there much cheaper ways we can train SAEs of acceptable quality? Or to get similar effects with other feature clustering or dictionary learning methods instead?* If we’re taking a pragmatic approach to SAEs, rather than the ambitious approach of trying to find the canonical units of analysis, then sacrificing some quality in return for lowering the major up front cost of SAE training may be worthwhile.
#+END_QUOTE

** Nurt 1 - zachłanne metody dekodowania i ich korzyści
:PROPERTIES:
:CREATED:  <2025-10-20 pon> [23:29]
:END:

*are there much cheaper ways we can train SAEs of acceptable quality? Or to get similar effects with other feature clustering or dictionary learning methods instead?*

Jedyną pracą która wykorzystuje zachłanne metody dekodowania jest ta z MPSAE https://arxiv.org/pdf/2506.05239v1
ale ta praca wykorzystuje tylko zachłanne dekodowanie /jako architekturę/, a nie jako coś co można użyć do wytrenowanego SAE.

*** Hipoteza: dzięki zachłannemu dekodowaniu można pominąć trenowanie dla wielu hiperparametrów
:PROPERTIES:
:CREATED:  <2025-10-20 pon> [23:34]
:END:

Opiera się to na prostej obserwacji że metodami zachłannymi możemy sprowadzić model z wysokim $k$, powiedzmy 500, do $k=100$

Eksperyment: sprawdzamy modele np TopK z różnym $k$ i porównujemy je do tego co otrzymaliśmy zachłannym dekodowaniem z modelu z najwyższym $k$



* Pomysły z architekturą
:PROPERTIES:
:CREATED:  <2025-10-20 pon> [23:19]
:END:

** DONE Reprodukcja modeli z SAEBench
:PROPERTIES:
:CREATED:  <2025-10-20 pon> [23:16]
:END:

Wygląda na to że modele do SAEBench były trenowane przez skrypt w dictionary_learning_demo

Oryginalnie to jest używane z 500M tokenów ale w sumie mnie bardziej podoba się pomysł żeby używać 5M (a potem może 50M) żeby przyspiszyć eksperymenty

** TODO Modele z pursuit/progowaniem
:PROPERTIES:
:CREATED:  <2025-10-20 pon> [23:19]
:END:
