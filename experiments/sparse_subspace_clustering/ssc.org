#+title: SSC

* Testing SSC implementation 
:PROPERTIES:
:CREATED:  <2025-12-29 Mon> [18:54]
:END:

Subspace clustering is an unsupervised method to identify clusters in high-dimensional data by grouping examples into subspaces.

This is achieved by finding a matrix $C$ such that $CX = X, diag(C) = 0$. Intuitively this /self-expressiveness/ - the fact that each example can be expressed as linear combination of other examples is a way to capture the fact that these examples do in fact share subspaces.

Sparse subspace clustering tries to additionally minimize $\|C\|_0$, the number of nonzero coefficients.

#+BEGIN_SRC python :session ssc.org  :exports both
import time
import pandas as pd
import numpy as np
from sae_research.sparse_subspace_clustering import SparseSubspaceClusteringOMP
#+END_SRC

#+RESULTS:
: None

** Synthetic data 
:PROPERTIES:
:CREATED:  <2025-12-29 Mon> [18:56]
:END:

Dataset setup: we will use the example from "Generalized Principal Component Analysis"

Consider 3 subspaces in R^3: the xy plane and lines z=x, z=-x

We'll sample 10 points from each of these subspaces

#+BEGIN_SRC python :session ssc.org  :exports both
np.random.seed(42)
n_points_per_subspace = 10

# Subspace 1: xy plane (z=0)
xy_plane = np.column_stack([
    np.random.randn(n_points_per_subspace),
    np.random.randn(n_points_per_subspace),
    np.zeros(n_points_per_subspace)
])

# Subspace 2: line z=x (y=0)
t2 = np.random.randn(n_points_per_subspace)
line_zx = np.column_stack([t2, np.zeros(n_points_per_subspace), t2])

# Subspace 3: line z=-x (y=0)
t3 = np.random.randn(n_points_per_subspace)
line_znx = np.column_stack([t3, np.zeros(n_points_per_subspace), -t3])

# Stack all points
X = np.vstack([xy_plane, line_zx, line_znx])
# True labels
labels = np.repeat([0, 1, 2], n_points_per_subspace)

X
#+END_SRC

#+RESULTS:
#+begin_example
array([[ 0.49671415, -0.46341769,  0.        ],
       [-0.1382643 , -0.46572975,  0.        ],
       [ 0.64768854,  0.24196227,  0.        ],
       [ 1.52302986, -1.91328024,  0.        ],
       [-0.23415337, -1.72491783,  0.        ],
       [-0.23413696, -0.56228753,  0.        ],
       [ 1.57921282, -1.01283112,  0.        ],
       [ 0.76743473,  0.31424733,  0.        ],
       [-0.46947439, -0.90802408,  0.        ],
       [ 0.54256004, -1.4123037 ,  0.        ],
       [ 1.46564877,  0.        ,  1.46564877],
       [-0.2257763 ,  0.        , -0.2257763 ],
       [ 0.0675282 ,  0.        ,  0.0675282 ],
       [-1.42474819,  0.        , -1.42474819],
       [-0.54438272,  0.        , -0.54438272],
       [ 0.11092259,  0.        ,  0.11092259],
       [-1.15099358,  0.        , -1.15099358],
       [ 0.37569802,  0.        ,  0.37569802],
       [-0.60063869,  0.        , -0.60063869],
       [-0.29169375,  0.        , -0.29169375],
       [-0.60170661,  0.        ,  0.60170661],
       [ 1.85227818,  0.        , -1.85227818],
       [-0.01349722,  0.        ,  0.01349722],
       [-1.05771093,  0.        ,  1.05771093],
       [ 0.82254491,  0.        , -0.82254491],
       [-1.22084365,  0.        ,  1.22084365],
       [ 0.2088636 ,  0.        , -0.2088636 ],
       [-1.95967012,  0.        ,  1.95967012],
       [-1.32818605,  0.        ,  1.32818605],
       [ 0.19686124,  0.        , -0.19686124]])
#+end_example

Let's see if SSC was able to correctly identify subspaces:

#+BEGIN_SRC python :session ssc.org  :exports both
start = time.time()
ssc = SparseSubspaceClusteringOMP(3, k=3, use_omp=True).fit(X)
f"Fit time: {time.time() - start:.3f}s"
#+END_SRC

#+RESULTS:
: Fit time: 0.012s


#+BEGIN_SRC python :session ssc.org  :exports both
ssc.labels_
#+END_SRC

#+RESULTS:
: array([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
:        1, 1, 1, 1, 1, 1, 1, 1], dtype=int32)

Since the ordering of labels doesn't mean anything, it means that SSC works. Another way to see it is to use Rand score

#+BEGIN_SRC python :session ssc.org  :exports both
from sklearn.metrics import adjusted_rand_score

adjusted_rand_score(ssc.labels_, labels)
#+END_SRC

#+RESULTS:
: 1.0

Now let's check the self-expressiveness

$CX = X$

We'll check $CX - X$

#+BEGIN_SRC python :session ssc.org  :exports both
(ssc._coefs.toarray() @ X) - X
#+END_SRC

#+RESULTS:
#+begin_example
array([[ 1.0471373 , -1.29938995,  0.        ],
       [-0.05971871, -0.10453181,  0.        ],
       [ 0.0683465 ,  0.01418203,  0.        ],
       [-1.12633954,  1.13686891,  0.        ],
       [ 0.34722559,  0.84456469,  0.        ],
       [ 0.1802543 , -0.48820672,  0.        ],
       [-1.02942829,  0.59674079,  0.        ],
       [-0.06835992, -0.01418481,  0.        ],
       [ 0.12672526,  0.3123502 ,  0.        ],
       [ 0.81278949, -0.46034523,  0.        ],
       [-0.87933101,  0.        , -0.87933101],
       [-0.36054146,  0.        , -0.36054146],
       [ 0.51878955,  0.        ,  0.51878955],
       [ 0.83843043,  0.        ,  0.83843043],
       [-0.04193503,  0.        , -0.04193503],
       [ 0.47539517,  0.        ,  0.47539517],
       [ 0.56467582,  0.        ,  0.56467582],
       [ 0.21061974,  0.        ,  0.21061974],
       [ 0.01432093,  0.        ,  0.01432093],
       [-0.29462401,  0.        , -0.29462401],
       [ 0.12063846,  0.        , -0.12063846],
       [-1.37121003,  0.        ,  1.37121003],
       [-0.46757093,  0.        ,  0.46757093],
       [ 0.57664277,  0.        , -0.57664277],
       [-0.34147676,  0.        ,  0.34147676],
       [ 0.73977549,  0.        , -0.73977549],
       [ 0.27220456,  0.        , -0.27220456],
       [ 1.47860197,  0.        , -1.47860197],
       [ 0.84711789,  0.        , -0.84711789],
       [ 0.28420692,  0.        , -0.28420692]])
#+end_example

That doesn't work but it's because *OMP normalizes columns*

#+BEGIN_SRC python :session ssc.org  :exports both
X_normed = X / np.linalg.norm(X, axis=1)[:, np.newaxis]

normed_diff = (ssc._coefs.toarray() @ X_normed) - X_normed
normed_diff
#+END_SRC

#+RESULTS:
#+begin_example
array([[-3.33066907e-16,  0.00000000e+00,  0.00000000e+00],
       [-2.77555756e-16, -1.11022302e-16,  0.00000000e+00],
       [-3.33066907e-16, -2.22044605e-16,  0.00000000e+00],
       [ 1.11022302e-16, -1.11022302e-16,  0.00000000e+00],
       [ 0.00000000e+00,  1.11022302e-16,  0.00000000e+00],
       [ 0.00000000e+00,  2.22044605e-16,  0.00000000e+00],
       [ 2.22044605e-16, -2.22044605e-16,  0.00000000e+00],
       [ 2.22044605e-16,  0.00000000e+00,  0.00000000e+00],
       [-4.99600361e-16, -4.44089210e-16,  0.00000000e+00],
       [ 5.55111512e-17, -1.11022302e-16,  0.00000000e+00],
       [ 1.11022302e-16,  0.00000000e+00,  1.11022302e-16],
       [-1.11022302e-16,  0.00000000e+00, -1.11022302e-16],
       [ 1.11022302e-16,  0.00000000e+00,  1.11022302e-16],
       [-2.22044605e-16,  0.00000000e+00, -2.22044605e-16],
       [-1.11022302e-16,  0.00000000e+00, -1.11022302e-16],
       [ 1.11022302e-16,  0.00000000e+00,  1.11022302e-16],
       [-2.22044605e-16,  0.00000000e+00, -2.22044605e-16],
       [ 2.22044605e-16,  0.00000000e+00,  2.22044605e-16],
       [-1.11022302e-16,  0.00000000e+00, -1.11022302e-16],
       [-2.22044605e-16,  0.00000000e+00, -2.22044605e-16],
       [-2.22044605e-16,  0.00000000e+00,  2.22044605e-16],
       [ 2.22044605e-16,  0.00000000e+00, -2.22044605e-16],
       [-1.11022302e-16,  0.00000000e+00,  1.11022302e-16],
       [-2.22044605e-16,  0.00000000e+00,  2.22044605e-16],
       [ 2.22044605e-16,  0.00000000e+00, -2.22044605e-16],
       [-1.11022302e-16,  0.00000000e+00,  1.11022302e-16],
       [ 1.11022302e-16,  0.00000000e+00, -1.11022302e-16],
       [-1.11022302e-16,  0.00000000e+00,  1.11022302e-16],
       [-1.11022302e-16,  0.00000000e+00,  1.11022302e-16],
       [ 1.11022302e-16,  0.00000000e+00, -1.11022302e-16]])
#+end_example

We can check that all the elements are close to zero:

#+BEGIN_SRC python :session ssc.org  :exports both
np.all(np.isclose(normed_diff, 0))
#+END_SRC

#+RESULTS:
: True


*** Gotchas 
:PROPERTIES:
:CREATED:  <2025-12-30 Tue> [21:18]
:END:

The SSC algorithm is used to cluster /linear/ subspaces - it can fail if these subspaces do not pass through 0 (they are /affine/ subspaces).

In practice though there is some evidence that this does not matter that much in high-dimensional settings, see [[cite:&you20_is_affin_const_needed_affin_subsp_clust]] where the authors show that the bigger the dimension of ambient space the less of a difference it makes.

#+BEGIN_QUOTE 
This paper shows, both the-
oretically and empirically, that when the dimension of the
ambient space is high relative to the sum of the dimensions
of the affine subspaces, the affine constraint has a negligi-
ble effect on clustering performance. Specifically, our anal-
ysis provides conditions that guarantee the correctness of
affine subspace clustering methods both with and without
the affine constraint, and shows that these conditions are
satisfied for high-dimensional data
#+END_QUOTE

*** More dimensions... 
:PROPERTIES:
:CREATED:  <2025-12-29 Mon> [19:24]
:END:

We'll use the same example but with a twist: bump number of points to 300 for 1st subspace, 50 for 2nd and 3rd and embed all this data into a larger 100d space so it's harder computationally

To do this we will create data like before and use a random orthogonal matrix to rotate this stuff into 100d 

#+BEGIN_SRC python :session ssc.org  :exports both
np.random.seed(42)
n_plane = 10000 
n_line = 1000
embed_dim = 5000 

# Subspace 1: xy plane (z=0) - 300 points
xy_plane_hd = np.column_stack([
    np.random.randn(n_plane),
    np.random.randn(n_plane),
    np.zeros(n_plane)
])

# Subspace 2: line z=x (y=0) - 50 points
t2 = np.random.randn(n_line)
line_zx_hd = np.column_stack([t2, np.zeros(n_line), t2])

# Subspace 3: line z=-x (y=0) - 50 points
t3 = np.random.randn(n_line)
line_znx_hd = np.column_stack([t3, np.zeros(n_line), -t3])

# Stack all points in 3D
X_3d = np.vstack([xy_plane_hd, line_zx_hd, line_znx_hd])
labels_hd = np.concatenate([np.zeros(n_plane), np.ones(n_line), 2*np.ones(n_line)])

# Random orthogonal matrix for embedding 3D -> 100D
# Use QR decomposition to get orthonormal columns
random_matrix = np.random.randn(embed_dim, 3)
Q, _ = np.linalg.qr(random_matrix)
embedding_matrix = Q[:, :3].T  # (3, 100)

# Embed into 100D
X_hd = X_3d @ embedding_matrix

X_hd.shape, labels_hd.shape
#+END_SRC

#+RESULTS:
| 12000 | 5000 |
| 12000 |      |

#+BEGIN_SRC python :session ssc.org  :exports both :async
start = time.time()
ssc_hd = SparseSubspaceClusteringOMP(3, k=25, batch_size=64, use_omp=True).fit(X_hd)
elapsed = time.time() - start
#+END_SRC

#+RESULTS:
: None

#+BEGIN_SRC python :session ssc.org  :exports both :async
f"Fit time: {int(elapsed // 60)}m {int(elapsed) % 60}s"
#+END_SRC

#+RESULTS:
: Fit time: 14m 11s

#+BEGIN_SRC python :session ssc.org  :exports both :async
pd.Series(ssc_hd.labels_).value_counts()
#+END_SRC

#+RESULTS:
: 0    10000
: 2     1000
: 1     1000
: Name: count, dtype: int64



** SAE features 
:PROPERTIES:
:CREATED:  <2025-12-30 Tue> [21:31]
:END:

These use Goodfire SAE for Llama-3.1-8B Instruct

#+BEGIN_SRC python :session ssc.org  :exports both :async
from huggingface_hub import hf_hub_download
import torch

sae_name = "Llama-3.1-8B-Instruct-SAE-l19"
file_path = hf_hub_download(
    repo_id=f"Goodfire/{sae_name}",
    filename=f"{sae_name}.pth",
    repo_type="model"
)
sae_params = torch.load(
    file_path, weights_only=True
)
sae_features_pt = sae_params["decoder_linear.weight"]
sae_features_pt.shape
#+END_SRC

#+RESULTS:
: torch.Size([4096, 65536])

#+BEGIN_SRC python :session ssc.org  :exports both
feature_labels = pd.read_csv("goodfire_sae_labels.csv")
feature_labels
#+END_SRC

#+RESULTS:
#+begin_example
       feature_index                                              label
0                  1  Syntactical special characters and delimiters ...
1                  2  The Russian word состав (composition/compilati...
2                  3  Syntactical markers that introduce description...
3                  4  The assistant is politely offering help using ...
4                  5  Technical documentation describing widespread ...
...              ...                                                ...
45413          65526  Traditional fables and fairy tales, particular...
45414          65527  Historical immigration to America and its cult...
45415          65528  The assistant is explaining programming concep...
45416          65530  Romance language grammar particles and connect...
45417          65532  Detailed urban description and city-focused na...

[45418 rows x 2 columns]
#+end_example

#+BEGIN_SRC python :session ssc.org  :exports both
sae_features_raw = sae_features_pt.T.cpu().numpy()
#+END_SRC

#+RESULTS:
: None

#+BEGIN_SRC python :session ssc.org  :exports both
feature_norm = pd.Series(np.linalg.norm(sae_features_raw, axis=1))
sae_features = sae_features_raw[feature_norm >= 1e-3]
sae_features.shape
#+END_SRC

#+RESULTS:
| 45418 | 4096 |

Ok so it seems like this number is consistent with what was used in interp-embed (apparently they also did the same filtering step)

#+BEGIN_SRC python :session ssc.org  :exports both :async
start = time.time()
elapsed = time.time() - start
#+END_SRC

#+RESULTS:
: Fit time: 4470.684s

#+BEGIN_SRC python :session ssc.org  :exports both
goodfire_sae_ssc._coefs 
#+END_SRC

#+RESULTS:
: <sae_research.sparse_subspace_clustering.SparseSubspaceClusteringOMP object at 0x131c1b5e0>

#+BEGIN_SRC python :session ssc.org  :exports both
import pickle
from pathlib import Path

output_path = Path("data/goodfire_sae_ssc_coefs.pkl")
if not output_path.exists():
    output_path.parent.mkdir(parents=True, exist_ok=True)
    goodfire_sae_ssc = SparseSubspaceClusteringOMP(50, k=25, batch_size=128).fit(sae_features)
    with open(output_path, "wb") as f:
        pickle.dump(goodfire_sae_ssc._coefs, f)
    print(f"Saved to {output_path}")
else:
    goodfire_sae_ssc = SparseSubspaceClusteringOMP(50, k=25, batch_size=128)
    with open(output_path, "rb") as f:
        goodfire_sae_ssc._coefs = pickle.load( f)
#+END_SRC

#+RESULTS:
: None

#+BEGIN_SRC python :session ssc.org  :exports both :async
start = time.time()
goodfire_sae_ssc = goodfire_sae_ssc.recompute_clustering(200)
elapsed = time.time() - start
#+END_SRC

#+RESULTS:
: None

#+BEGIN_SRC python :session ssc.org  :exports both :async
f"Fit time: {int(elapsed // 60)}m {int(elapsed) % 60}s"
#+END_SRC

#+RESULTS:
: Fit time: 0m 54s

#+BEGIN_SRC python :session ssc.org  :exports both
feature_labels_with_clusters = feature_labels.copy().assign(cluster=goodfire_sae_ssc.labels_)
feature_labels_with_clusters["cluster"].value_counts()
#+END_SRC

#+RESULTS:
#+begin_example
cluster
75     7802
5      2617
150    2556
3      2177
78     1616
       ... 
25       23
188      21
41       20
74       18
13       17
Name: count, Length: 200, dtype: int64
#+end_example

#+BEGIN_SRC python :session ssc.org  :exports both
feature_labels_with_clusters["cluster"].value_counts().describe()
#+END_SRC

#+RESULTS:
: count     200.000000
: mean      227.090000
: std       637.833481
: min        17.000000
: 25%        47.000000
: 50%        96.000000
: 75%       190.500000
: max      7802.000000
: Name: count, dtype: float64

#+BEGIN_SRC python :session ssc.org  :exports both
feature_labels_with_clusters[feature_labels_with_clusters["cluster"] == 1]
#+END_SRC

#+RESULTS:
#+begin_example
       feature_index                                              label  cluster
221              262  The assistant is providing a structured multi-...        1
931             1128   Carbon market mechanisms and trading instruments        1
1711            2106       Words describing suffering, horror and chaos        1
3057            4050      Volunteering and giving back to the community        1
3123            4161  The assistant should explain and defend the sc...        1
...              ...                                                ...      ...
42133          60769  Environmental sustainability in industrial pro...        1
42143          60781  Environmental sustainability and clean energy ...        1
42227          60896  Comparative environmental impacts of energy so...        1
42519          61313  Discussions of sustainability and environmenta...        1
43610          62894        Future projections and timeline-based goals        1

[73 rows x 3 columns]
#+end_example

#+BEGIN_SRC python :session ssc.org  :exports both
feature_labels_with_clusters.to_csv("goodfire_sae_labels_ssc_c200_k25.csv", index=False)
#+END_SRC

#+RESULTS:
: None
