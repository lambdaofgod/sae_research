#+title: SSC

* Testing SSC implementation 
:PROPERTIES:
:CREATED:  <2025-12-29 Mon> [18:54]
:END:

Subspace clustering is an unsupervised method to identify clusters in high-dimensional data by grouping examples into subspaces.

This is achieved by finding a matrix $C$ such that $CX = X, diag(C) = 0$. Intuitively this /self-expressiveness/ - the fact that each example can be expressed as linear combination of other examples is a way to capture the fact that these examples do in fact share subspaces.

Sparse subspace clustering tries to additionally minimize $\|C\|_0$, the number of nonzero coefficients.

#+BEGIN_SRC python :session ssc.org  :exports both
import time
import pandas as pd
import numpy as np
from sae_research.feature_families import SparseSubspaceClusteringOMP, CosineSimilarityClustering 
#+END_SRC

#+RESULTS:
: None

** Synthetic data 
:PROPERTIES:
:CREATED:  <2025-12-29 Mon> [18:56]
:END:

Dataset setup: we will use the example from "Generalized Principal Component Analysis"

Consider 3 subspaces in R^3: the xy plane and lines z=x, z=-x

We'll sample 10 points from each of these subspaces

#+BEGIN_SRC python :session ssc.org  :exports both
np.random.seed(42)
n_points_per_subspace = 10

# Subspace 1: xy plane (z=0)
xy_plane = np.column_stack([
    np.random.randn(n_points_per_subspace),
    np.random.randn(n_points_per_subspace),
    np.zeros(n_points_per_subspace)
])

# Subspace 2: line z=x (y=0)
t2 = np.random.randn(n_points_per_subspace)
line_zx = np.column_stack([t2, np.zeros(n_points_per_subspace), t2])

# Subspace 3: line z=-x (y=0)
t3 = np.random.randn(n_points_per_subspace)
line_znx = np.column_stack([t3, np.zeros(n_points_per_subspace), -t3])

# Stack all points
X = np.vstack([xy_plane, line_zx, line_znx])
# True labels
labels = np.repeat([0, 1, 2], n_points_per_subspace)

X
#+END_SRC

#+RESULTS:
#+begin_example
array([[ 0.49671415, -0.46341769,  0.        ],
       [-0.1382643 , -0.46572975,  0.        ],
       [ 0.64768854,  0.24196227,  0.        ],
       [ 1.52302986, -1.91328024,  0.        ],
       [-0.23415337, -1.72491783,  0.        ],
       [-0.23413696, -0.56228753,  0.        ],
       [ 1.57921282, -1.01283112,  0.        ],
       [ 0.76743473,  0.31424733,  0.        ],
       [-0.46947439, -0.90802408,  0.        ],
       [ 0.54256004, -1.4123037 ,  0.        ],
       [ 1.46564877,  0.        ,  1.46564877],
       [-0.2257763 ,  0.        , -0.2257763 ],
       [ 0.0675282 ,  0.        ,  0.0675282 ],
       [-1.42474819,  0.        , -1.42474819],
       [-0.54438272,  0.        , -0.54438272],
       [ 0.11092259,  0.        ,  0.11092259],
       [-1.15099358,  0.        , -1.15099358],
       [ 0.37569802,  0.        ,  0.37569802],
       [-0.60063869,  0.        , -0.60063869],
       [-0.29169375,  0.        , -0.29169375],
       [-0.60170661,  0.        ,  0.60170661],
       [ 1.85227818,  0.        , -1.85227818],
       [-0.01349722,  0.        ,  0.01349722],
       [-1.05771093,  0.        ,  1.05771093],
       [ 0.82254491,  0.        , -0.82254491],
       [-1.22084365,  0.        ,  1.22084365],
       [ 0.2088636 ,  0.        , -0.2088636 ],
       [-1.95967012,  0.        ,  1.95967012],
       [-1.32818605,  0.        ,  1.32818605],
       [ 0.19686124,  0.        , -0.19686124]])
#+end_example

Let's see if SSC was able to correctly identify subspaces:

#+BEGIN_SRC python :session ssc.org  :exports both
start = time.time()
ssc = SparseSubspaceClusteringOMP(3, k=3, use_omp=True).fit(X)
f"Fit time: {time.time() - start:.3f}s"
#+END_SRC

#+RESULTS:
: Fit time: 0.012s


#+BEGIN_SRC python :session ssc.org  :exports both
ssc.labels_
#+END_SRC

#+RESULTS:
: array([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
:        1, 1, 1, 1, 1, 1, 1, 1], dtype=int32)

Since the ordering of labels doesn't mean anything, it means that SSC works. Another way to see it is to use Rand score

#+BEGIN_SRC python :session ssc.org  :exports both
from sklearn.metrics import adjusted_rand_score

adjusted_rand_score(ssc.labels_, labels)
#+END_SRC

#+RESULTS:
: 1.0

Now let's check the self-expressiveness

$CX = X$

We'll check $CX - X$

#+BEGIN_SRC python :session ssc.org  :exports both
(ssc._coefs.toarray() @ X) - X
#+END_SRC

#+RESULTS:
#+begin_example
array([[ 1.0471373 , -1.29938995,  0.        ],
       [-0.05971871, -0.10453181,  0.        ],
       [ 0.0683465 ,  0.01418203,  0.        ],
       [-1.12633954,  1.13686891,  0.        ],
       [ 0.34722559,  0.84456469,  0.        ],
       [ 0.1802543 , -0.48820672,  0.        ],
       [-1.02942829,  0.59674079,  0.        ],
       [-0.06835992, -0.01418481,  0.        ],
       [ 0.12672526,  0.3123502 ,  0.        ],
       [ 0.81278949, -0.46034523,  0.        ],
       [-0.87933101,  0.        , -0.87933101],
       [-0.36054146,  0.        , -0.36054146],
       [ 0.51878955,  0.        ,  0.51878955],
       [ 0.83843043,  0.        ,  0.83843043],
       [-0.04193503,  0.        , -0.04193503],
       [ 0.47539517,  0.        ,  0.47539517],
       [ 0.56467582,  0.        ,  0.56467582],
       [ 0.21061974,  0.        ,  0.21061974],
       [ 0.01432093,  0.        ,  0.01432093],
       [-0.29462401,  0.        , -0.29462401],
       [ 0.12063846,  0.        , -0.12063846],
       [-1.37121003,  0.        ,  1.37121003],
       [-0.46757093,  0.        ,  0.46757093],
       [ 0.57664277,  0.        , -0.57664277],
       [-0.34147676,  0.        ,  0.34147676],
       [ 0.73977549,  0.        , -0.73977549],
       [ 0.27220456,  0.        , -0.27220456],
       [ 1.47860197,  0.        , -1.47860197],
       [ 0.84711789,  0.        , -0.84711789],
       [ 0.28420692,  0.        , -0.28420692]])
#+end_example

That doesn't work but it's because *OMP normalizes columns*

#+BEGIN_SRC python :session ssc.org  :exports both
X_normed = X / np.linalg.norm(X, axis=1)[:, np.newaxis]

normed_diff = (ssc._coefs.toarray() @ X_normed) - X_normed
normed_diff
#+END_SRC

#+RESULTS:
#+begin_example
array([[-3.33066907e-16,  0.00000000e+00,  0.00000000e+00],
       [-2.77555756e-16, -1.11022302e-16,  0.00000000e+00],
       [-3.33066907e-16, -2.22044605e-16,  0.00000000e+00],
       [ 1.11022302e-16, -1.11022302e-16,  0.00000000e+00],
       [ 0.00000000e+00,  1.11022302e-16,  0.00000000e+00],
       [ 0.00000000e+00,  2.22044605e-16,  0.00000000e+00],
       [ 2.22044605e-16, -2.22044605e-16,  0.00000000e+00],
       [ 2.22044605e-16,  0.00000000e+00,  0.00000000e+00],
       [-4.99600361e-16, -4.44089210e-16,  0.00000000e+00],
       [ 5.55111512e-17, -1.11022302e-16,  0.00000000e+00],
       [ 1.11022302e-16,  0.00000000e+00,  1.11022302e-16],
       [-1.11022302e-16,  0.00000000e+00, -1.11022302e-16],
       [ 1.11022302e-16,  0.00000000e+00,  1.11022302e-16],
       [-2.22044605e-16,  0.00000000e+00, -2.22044605e-16],
       [-1.11022302e-16,  0.00000000e+00, -1.11022302e-16],
       [ 1.11022302e-16,  0.00000000e+00,  1.11022302e-16],
       [-2.22044605e-16,  0.00000000e+00, -2.22044605e-16],
       [ 2.22044605e-16,  0.00000000e+00,  2.22044605e-16],
       [-1.11022302e-16,  0.00000000e+00, -1.11022302e-16],
       [-2.22044605e-16,  0.00000000e+00, -2.22044605e-16],
       [-2.22044605e-16,  0.00000000e+00,  2.22044605e-16],
       [ 2.22044605e-16,  0.00000000e+00, -2.22044605e-16],
       [-1.11022302e-16,  0.00000000e+00,  1.11022302e-16],
       [-2.22044605e-16,  0.00000000e+00,  2.22044605e-16],
       [ 2.22044605e-16,  0.00000000e+00, -2.22044605e-16],
       [-1.11022302e-16,  0.00000000e+00,  1.11022302e-16],
       [ 1.11022302e-16,  0.00000000e+00, -1.11022302e-16],
       [-1.11022302e-16,  0.00000000e+00,  1.11022302e-16],
       [-1.11022302e-16,  0.00000000e+00,  1.11022302e-16],
       [ 1.11022302e-16,  0.00000000e+00, -1.11022302e-16]])
#+end_example

We can check that all the elements are close to zero:

#+BEGIN_SRC python :session ssc.org  :exports both
np.all(np.isclose(normed_diff, 0))
#+END_SRC

#+RESULTS:
: True


*** Gotchas 
:PROPERTIES:
:CREATED:  <2025-12-30 Tue> [21:18]
:END:

The SSC algorithm is used to cluster /linear/ subspaces - it can fail if these subspaces do not pass through 0 (they are /affine/ subspaces).

In practice though there is some evidence that this does not matter that much in high-dimensional settings, see [[cite:&you20_is_affin_const_needed_affin_subsp_clust]] where the authors show that the bigger the dimension of ambient space the less of a difference it makes.

#+BEGIN_QUOTE 
This paper shows, both the-
oretically and empirically, that when the dimension of the
ambient space is high relative to the sum of the dimensions
of the affine subspaces, the affine constraint has a negligi-
ble effect on clustering performance. Specifically, our anal-
ysis provides conditions that guarantee the correctness of
affine subspace clustering methods both with and without
the affine constraint, and shows that these conditions are
satisfied for high-dimensional data
#+END_QUOTE

*** More dimensions... 
:PROPERTIES:
:CREATED:  <2025-12-29 Mon> [19:24]
:END:

We'll use the same example but with a twist: bump number of points to 300 for 1st subspace, 50 for 2nd and 3rd and embed all this data into a larger 100d space so it's harder computationally

To do this we will create data like before and use a random orthogonal matrix to rotate this stuff into 100d 

#+BEGIN_SRC python :session ssc.org  :exports both
np.random.seed(42)
n_plane = 10000 
n_line = 1000
embed_dim = 5000 

# Subspace 1: xy plane (z=0) - 300 points
xy_plane_hd = np.column_stack([
    np.random.randn(n_plane),
    np.random.randn(n_plane),
    np.zeros(n_plane)
])

# Subspace 2: line z=x (y=0) - 50 points
t2 = np.random.randn(n_line)
line_zx_hd = np.column_stack([t2, np.zeros(n_line), t2])

# Subspace 3: line z=-x (y=0) - 50 points
t3 = np.random.randn(n_line)
line_znx_hd = np.column_stack([t3, np.zeros(n_line), -t3])

# Stack all points in 3D
X_3d = np.vstack([xy_plane_hd, line_zx_hd, line_znx_hd])
labels_hd = np.concatenate([np.zeros(n_plane), np.ones(n_line), 2*np.ones(n_line)])

# Random orthogonal matrix for embedding 3D -> 100D
# Use QR decomposition to get orthonormal columns
random_matrix = np.random.randn(embed_dim, 3)
Q, _ = np.linalg.qr(random_matrix)
embedding_matrix = Q[:, :3].T  # (3, 100)

# Embed into 100D
X_hd = X_3d @ embedding_matrix

X_hd.shape, labels_hd.shape
#+END_SRC

#+RESULTS:
| 12000 | 5000 |
| 12000 |      |

#+BEGIN_SRC python :session ssc.org  :exports both :async
start = time.time()
ssc_hd = SparseSubspaceClusteringOMP(3, k=25, batch_size=64, use_omp=True).fit(X_hd)
elapsed = time.time() - start
#+END_SRC

#+RESULTS:
: None

#+BEGIN_SRC python :session ssc.org  :exports both :async
f"Fit time: {int(elapsed // 60)}m {int(elapsed) % 60}s"
#+END_SRC

#+RESULTS:
: Fit time: 14m 11s

#+BEGIN_SRC python :session ssc.org  :exports both :async
pd.Series(ssc_hd.labels_).value_counts()
#+END_SRC

#+RESULTS:
: /var/folders/t7/9zd9yqf17_zbfq2hwm8tv9ym0000gn/T/babel-d7wG0u/python-udsQAW



** SAE features 
:PROPERTIES:
:CREATED:  <2025-12-30 Tue> [21:31]
:END:

These use Goodfire SAE for Llama-3.1-8B Instruct

#+BEGIN_SRC python :session ssc.org  :exports both :async
from sae_research.sae_features import SAEFeatures

sae_features = SAEFeatures.from_goodfire_hf(
    sae_name="Llama-3.1-8B-Instruct-SAE-l19",
    labels_path="goodfire_sae_labels.csv"
)
sae_features.features.shape
#+END_SRC

#+RESULTS:
| 45418 | 4096 |

Ok so it seems like this number is consistent with what was used in interp-embed (apparently they also did the same filtering step)

*** Simpler cosine-similarity based clustering 
:PROPERTIES:
:CREATED:  <2026-01-03 Sat> [18:20]
:END:

T = 0.1

#+BEGIN_SRC python :session ssc.org  :exports both :async
start = time.time()
goodfire_sae_csc = CosineSimilarityClustering(threshold=0.5, n_clusters=300).fit(sae_features.features)
elapsed = time.time() - start
#+END_SRC

#+RESULTS:
: None

#+BEGIN_SRC python :session ssc.org  :exports both :async
pd.Series(goodfire_sae_csc.labels_).value_counts()
#+END_SRC

#+RESULTS:
#+begin_example
1     40368
22      298
16      259
36      254
5       242
45      240
48      223
49      206
7       198
17      191
44      186
42      181
8       179
26      162
37      160
27      150
35      146
15      140
47      140
32      127
39      125
19      122
11      121
38      114
4       112
46      107
20      103
43       99
30       73
18       65
9        60
3        57
28       51
12       34
10       33
21       28
23       25
0        13
14        4
40        2
6         2
29        2
34        2
33        2
41        2
24        2
2         2
25        2
13        2
31        2
Name: count, dtype: int64
#+end_example

T = 0.05
#+BEGIN_SRC python :session ssc.org  :exports both :async
goodfire_sae_csc = CosineSimilarityClustering(threshold=0.05).fit(sae_features.features)
pd.Series(goodfire_sae_csc.labels_).value_counts()
#+END_SRC

#+RESULTS:
: /var/folders/t7/9zd9yqf17_zbfq2hwm8tv9ym0000gn/T/babel-d7wG0u/python-m1QGbY


T = 0.025
#+BEGIN_SRC python :session ssc.org  :exports both :async
goodfire_sae_csc = CosineSimilarityClustering(threshold=0.025).fit(sae_features.features)
pd.Series(goodfire_sae_csc.labels_).value_counts()
#+END_SRC

#+BEGIN_SRC python :session ssc.org  :exports both

#+END_SRC

#+BEGIN_SRC python :session ssc.org  :exports both :async
f"Fit time: {int(elapsed // 60)}m {int(elapsed) % 60}s"
#+END_SRC

#+RESULTS:
: Fit time: 10m 48s

#+BEGIN_SRC python :session ssc.org  :exports both
pd.Series(goodfire_sae_csc.labels_).value_counts()
#+END_SRC

#+BEGIN_SRC python :session ssc.org  :exports both
example_sims = goodfire_sae_csc._thresholded_similarities[0].toarray().reshape(-1)
#pd.Series(.describe()
pd.Series(example_sims[example_sims > 0]).describe()
#+END_SRC

#+RESULTS:
: count    16326.000000
: mean         0.033812
: std          0.028965
: min          0.010000
: 25%          0.016232
: 50%          0.025014
: 75%          0.040690
: max          1.000001
: dtype: float64

#+BEGIN_SRC python :session ssc.org  :exports both
pd.Series(goodfire_sae_csc.labels_).value_counts()
#+END_SRC

#+RESULTS:
: 0    45418
: Name: count, dtype: int64

*** Sparse Subspace Clustering 
:PROPERTIES:
:CREATED:  <2026-01-03 Sat> [18:20]
:END:

#+BEGIN_SRC python :session ssc.org  :exports both
goodfire_sae_ssc._coefs 
#+END_SRC

#+RESULTS:
: <sae_research.sparse_subspace_clustering.SparseSubspaceClusteringOMP object at 0x131c1b5e0>

We select these numbers so that their product approximately equals feature dimensionality

#+BEGIN_SRC python :session ssc.org  :exports both
n_clusters = 400 
k = 10
#+END_SRC

#+RESULTS:
: None

#+BEGIN_SRC python :session ssc.org  :exports both :async
import pickle
from pathlib import Path

goodfire_sae_ssc = SparseSubspaceClusteringOMP(n_clusters, k=k, batch_size=128)

output_path = Path(f"data/goodfire_sae_ssc_coefs_k{k}.pkl")
if not output_path.exists():
    output_path.parent.mkdir(parents=True, exist_ok=True)
    goodfire_sae_ssc.fit(sae_features.features)
    with open(output_path, "wb") as f:
        pickle.dump(goodfire_sae_ssc._coefs, f)
    print(f"Saved to {output_path}")
else:
    with open(output_path, "rb") as f:
        goodfire_sae_ssc._coefs = pickle.load( f)
#+END_SRC

#+RESULTS:
: None

#+BEGIN_SRC python :session ssc.org  :exports both :async
start = time.time()
goodfire_sae_ssc = goodfire_sae_ssc.recompute_clustering(n_clusters)
elapsed = time.time() - start
#+END_SRC

#+RESULTS:
: None

#+BEGIN_SRC python :session ssc.org  :exports both :async
f"Fit time: {int(elapsed // 60)}m {int(elapsed) % 60}s"
#+END_SRC

#+RESULTS:
: Fit time: 2m 0s

#+BEGIN_SRC python :session ssc.org  :exports both
pd.Series(goodfire_sae_ssc.labels_).value_counts()
#+END_SRC

#+RESULTS:
#+begin_example
135    8499
255    3740
28     2424
1      1366
353    1291
       ... 
239       8
33        7
40        7
31        7
0         6
Name: count, Length: 400, dtype: int64
#+end_example

#+BEGIN_SRC python :session ssc.org  :exports both
feature_labels_with_clusters = sae_features.labels.copy().assign(cluster=goodfire_sae_ssc.labels_)
feature_labels_with_clusters["cluster"].value_counts()
#+END_SRC

#+RESULTS:
#+begin_example
cluster
206    11201
6       2954
245     2030
86      1831
99      1426
       ...  
16         9
125        8
65         8
57         8
142        7
Name: count, Length: 300, dtype: int64
#+end_example

#+BEGIN_SRC python :session ssc.org  :exports both
feature_labels_with_clusters["cluster"].value_counts().describe()
#+END_SRC

#+RESULTS:
: count      300.000000
: mean       151.393333
: std        693.357971
: min          7.000000
: 25%         23.750000
: 50%         43.000000
: 75%        109.250000
: max      11201.000000
: Name: count, dtype: float64

#+BEGIN_SRC python :session ssc.org  :exports both
feature_labels_with_clusters[feature_labels_with_clusters["cluster"] == 1]
#+END_SRC

#+RESULTS:
#+begin_example
       feature_index                                              label  cluster
40                49                    Offensive request from the user        1
161              192      Nuanced disagreement with absolute statements        1
227              269  Content restriction clauses in instruction pro...        1
434              512  Markers of generic or low-quality content that...        1
495              589  User requests for comma-separated list with no...        1
...              ...                                                ...      ...
44511          64222  Expressions of limitation, inadequacy or falli...        1
44593          64351  The assistant is correcting or being corrected...        1
44708          64516  Narrative contradiction and expectation subver...        1
44772          64608  Strong denial or defiance in confrontational d...        1
44883          64753       Failure to take required or expected actions        1

[219 rows x 3 columns]
#+end_example

#+BEGIN_SRC python :session ssc.org  :exports both
feature_labels_with_clusters.to_csv("goodfire_sae_labels_ssc_c300_k10.csv", index=False)
#+END_SRC

#+RESULTS:
: None

#+BEGIN_SRC python :session ssc.org  :exports both
f
#+END_SRC
