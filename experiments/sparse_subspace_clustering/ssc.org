#+title: SSC

* Testing SSC implementation 
:PROPERTIES:
:CREATED:  <2025-12-29 Mon> [18:54]
:END:

Subspace clustering is an unsupervised method to identify clusters in high-dimensional data by grouping examples into subspaces.

This is achieved by finding a matrix $C$ such that $CX = X, diag(C) = 0$. Intuitively this /self-expressiveness/ - the fact that each example can be expressed as linear combination of other examples is a way to capture the fact that these examples do in fact share subspaces.

Sparse subspace clustering tries to additionally minimize $\|C\|_0$, the number of nonzero coefficients.

#+BEGIN_SRC python :session ssc.org  :exports both
import time
import pandas as pd
import numpy as np
from sae_research.sparse_subspace_clustering import SparseSubspaceClusteringOMP
#+END_SRC

#+RESULTS:
: None

** Synthetic data 
:PROPERTIES:
:CREATED:  <2025-12-29 Mon> [18:56]
:END:

Dataset setup: we will use the example from "Generalized Principal Component Analysis"

Consider 3 subspaces in R^3: the xy plane and lines z=x, z=-x

We'll sample 10 points from each of these subspaces

#+BEGIN_SRC python :session ssc.org  :exports both
np.random.seed(42)
n_points_per_subspace = 10

# Subspace 1: xy plane (z=0)
xy_plane = np.column_stack([
    np.random.randn(n_points_per_subspace),
    np.random.randn(n_points_per_subspace),
    np.zeros(n_points_per_subspace)
])

# Subspace 2: line z=x (y=0)
t2 = np.random.randn(n_points_per_subspace)
line_zx = np.column_stack([t2, np.zeros(n_points_per_subspace), t2])

# Subspace 3: line z=-x (y=0)
t3 = np.random.randn(n_points_per_subspace)
line_znx = np.column_stack([t3, np.zeros(n_points_per_subspace), -t3])

# Stack all points
X = np.vstack([xy_plane, line_zx, line_znx])
# True labels
labels = np.repeat([0, 1, 2], n_points_per_subspace)

X
#+END_SRC

#+RESULTS:
#+begin_example
array([[ 0.49671415, -0.46341769,  0.        ],
       [-0.1382643 , -0.46572975,  0.        ],
       [ 0.64768854,  0.24196227,  0.        ],
       [ 1.52302986, -1.91328024,  0.        ],
       [-0.23415337, -1.72491783,  0.        ],
       [-0.23413696, -0.56228753,  0.        ],
       [ 1.57921282, -1.01283112,  0.        ],
       [ 0.76743473,  0.31424733,  0.        ],
       [-0.46947439, -0.90802408,  0.        ],
       [ 0.54256004, -1.4123037 ,  0.        ],
       [ 1.46564877,  0.        ,  1.46564877],
       [-0.2257763 ,  0.        , -0.2257763 ],
       [ 0.0675282 ,  0.        ,  0.0675282 ],
       [-1.42474819,  0.        , -1.42474819],
       [-0.54438272,  0.        , -0.54438272],
       [ 0.11092259,  0.        ,  0.11092259],
       [-1.15099358,  0.        , -1.15099358],
       [ 0.37569802,  0.        ,  0.37569802],
       [-0.60063869,  0.        , -0.60063869],
       [-0.29169375,  0.        , -0.29169375],
       [-0.60170661,  0.        ,  0.60170661],
       [ 1.85227818,  0.        , -1.85227818],
       [-0.01349722,  0.        ,  0.01349722],
       [-1.05771093,  0.        ,  1.05771093],
       [ 0.82254491,  0.        , -0.82254491],
       [-1.22084365,  0.        ,  1.22084365],
       [ 0.2088636 ,  0.        , -0.2088636 ],
       [-1.95967012,  0.        ,  1.95967012],
       [-1.32818605,  0.        ,  1.32818605],
       [ 0.19686124,  0.        , -0.19686124]])
#+end_example

Let's see if SSC was able to correctly identify subspaces:

#+BEGIN_SRC python :session ssc.org  :exports both
start = time.time()
ssc = SparseSubspaceClusteringOMP(3, k=3, use_omp=True).fit(X)
f"Fit time: {time.time() - start:.3f}s"
#+END_SRC

#+RESULTS:
: Fit time: 0.012s


#+BEGIN_SRC python :session ssc.org  :exports both
ssc.labels_
#+END_SRC

#+RESULTS:
: array([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
:        1, 1, 1, 1, 1, 1, 1, 1], dtype=int32)

Since the ordering of labels doesn't mean anything, it means that SSC works. Another way to see it is to use Rand score

#+BEGIN_SRC python :session ssc.org  :exports both
from sklearn.metrics import adjusted_rand_score

adjusted_rand_score(ssc.labels_, labels)
#+END_SRC

#+RESULTS:
: 1.0

Now let's check the self-expressiveness

$CX = X$

We'll check $CX - X$

#+BEGIN_SRC python :session ssc.org  :exports both
(ssc._coefs.toarray() @ X) - X
#+END_SRC

#+RESULTS:
#+begin_example
array([[ 1.0471373 , -1.29938995,  0.        ],
       [-0.05971871, -0.10453181,  0.        ],
       [ 0.0683465 ,  0.01418203,  0.        ],
       [-1.12633954,  1.13686891,  0.        ],
       [ 0.34722559,  0.84456469,  0.        ],
       [ 0.1802543 , -0.48820672,  0.        ],
       [-1.02942829,  0.59674079,  0.        ],
       [-0.06835992, -0.01418481,  0.        ],
       [ 0.12672526,  0.3123502 ,  0.        ],
       [ 0.81278949, -0.46034523,  0.        ],
       [-0.87933101,  0.        , -0.87933101],
       [-0.36054146,  0.        , -0.36054146],
       [ 0.51878955,  0.        ,  0.51878955],
       [ 0.83843043,  0.        ,  0.83843043],
       [-0.04193503,  0.        , -0.04193503],
       [ 0.47539517,  0.        ,  0.47539517],
       [ 0.56467582,  0.        ,  0.56467582],
       [ 0.21061974,  0.        ,  0.21061974],
       [ 0.01432093,  0.        ,  0.01432093],
       [-0.29462401,  0.        , -0.29462401],
       [ 0.12063846,  0.        , -0.12063846],
       [-1.37121003,  0.        ,  1.37121003],
       [-0.46757093,  0.        ,  0.46757093],
       [ 0.57664277,  0.        , -0.57664277],
       [-0.34147676,  0.        ,  0.34147676],
       [ 0.73977549,  0.        , -0.73977549],
       [ 0.27220456,  0.        , -0.27220456],
       [ 1.47860197,  0.        , -1.47860197],
       [ 0.84711789,  0.        , -0.84711789],
       [ 0.28420692,  0.        , -0.28420692]])
#+end_example

That doesn't work but it's because *OMP normalizes columns*

#+BEGIN_SRC python :session ssc.org  :exports both
X_normed = X / np.linalg.norm(X, axis=1)[:, np.newaxis]

normed_diff = (ssc._coefs.toarray() @ X_normed) - X_normed
normed_diff
#+END_SRC

#+RESULTS:
#+begin_example
array([[-3.33066907e-16,  0.00000000e+00,  0.00000000e+00],
       [-2.77555756e-16, -1.11022302e-16,  0.00000000e+00],
       [-3.33066907e-16, -2.22044605e-16,  0.00000000e+00],
       [ 1.11022302e-16, -1.11022302e-16,  0.00000000e+00],
       [ 0.00000000e+00,  1.11022302e-16,  0.00000000e+00],
       [ 0.00000000e+00,  2.22044605e-16,  0.00000000e+00],
       [ 2.22044605e-16, -2.22044605e-16,  0.00000000e+00],
       [ 2.22044605e-16,  0.00000000e+00,  0.00000000e+00],
       [-4.99600361e-16, -4.44089210e-16,  0.00000000e+00],
       [ 5.55111512e-17, -1.11022302e-16,  0.00000000e+00],
       [ 1.11022302e-16,  0.00000000e+00,  1.11022302e-16],
       [-1.11022302e-16,  0.00000000e+00, -1.11022302e-16],
       [ 1.11022302e-16,  0.00000000e+00,  1.11022302e-16],
       [-2.22044605e-16,  0.00000000e+00, -2.22044605e-16],
       [-1.11022302e-16,  0.00000000e+00, -1.11022302e-16],
       [ 1.11022302e-16,  0.00000000e+00,  1.11022302e-16],
       [-2.22044605e-16,  0.00000000e+00, -2.22044605e-16],
       [ 2.22044605e-16,  0.00000000e+00,  2.22044605e-16],
       [-1.11022302e-16,  0.00000000e+00, -1.11022302e-16],
       [-2.22044605e-16,  0.00000000e+00, -2.22044605e-16],
       [-2.22044605e-16,  0.00000000e+00,  2.22044605e-16],
       [ 2.22044605e-16,  0.00000000e+00, -2.22044605e-16],
       [-1.11022302e-16,  0.00000000e+00,  1.11022302e-16],
       [-2.22044605e-16,  0.00000000e+00,  2.22044605e-16],
       [ 2.22044605e-16,  0.00000000e+00, -2.22044605e-16],
       [-1.11022302e-16,  0.00000000e+00,  1.11022302e-16],
       [ 1.11022302e-16,  0.00000000e+00, -1.11022302e-16],
       [-1.11022302e-16,  0.00000000e+00,  1.11022302e-16],
       [-1.11022302e-16,  0.00000000e+00,  1.11022302e-16],
       [ 1.11022302e-16,  0.00000000e+00, -1.11022302e-16]])
#+end_example

We can check that all the elements are close to zero:

#+BEGIN_SRC python :session ssc.org  :exports both
np.all(np.isclose(normed_diff, 0))
#+END_SRC

#+RESULTS:
: True


*** Gotchas 
:PROPERTIES:
:CREATED:  <2025-12-30 Tue> [21:18]
:END:

The SSC algorithm is used to cluster /linear/ subspaces - it can fail if these subspaces do not pass through 0 (they are /affine/ subspaces).

In practice though there is some evidence that this does not matter that much in high-dimensional settings, see [[cite:&you20_is_affin_const_needed_affin_subsp_clust]] where the authors show that the bigger the dimension of ambient space the less of a difference it makes.

#+BEGIN_QUOTE 
This paper shows, both the-
oretically and empirically, that when the dimension of the
ambient space is high relative to the sum of the dimensions
of the affine subspaces, the affine constraint has a negligi-
ble effect on clustering performance. Specifically, our anal-
ysis provides conditions that guarantee the correctness of
affine subspace clustering methods both with and without
the affine constraint, and shows that these conditions are
satisfied for high-dimensional data
#+END_QUOTE

*** More dimensions... 
:PROPERTIES:
:CREATED:  <2025-12-29 Mon> [19:24]
:END:

We'll use the same example but with a twist: bump number of points to 300 for 1st subspace, 50 for 2nd and 3rd and embed all this data into a larger 100d space so it's harder computationally

To do this we will create data like before and use a random orthogonal matrix to rotate this stuff into 100d 

#+BEGIN_SRC python :session ssc.org  :exports both
np.random.seed(42)
n_plane = 10000 
n_line = 1000
embed_dim = 5000 

# Subspace 1: xy plane (z=0) - 300 points
xy_plane_hd = np.column_stack([
    np.random.randn(n_plane),
    np.random.randn(n_plane),
    np.zeros(n_plane)
])

# Subspace 2: line z=x (y=0) - 50 points
t2 = np.random.randn(n_line)
line_zx_hd = np.column_stack([t2, np.zeros(n_line), t2])

# Subspace 3: line z=-x (y=0) - 50 points
t3 = np.random.randn(n_line)
line_znx_hd = np.column_stack([t3, np.zeros(n_line), -t3])

# Stack all points in 3D
X_3d = np.vstack([xy_plane_hd, line_zx_hd, line_znx_hd])
labels_hd = np.concatenate([np.zeros(n_plane), np.ones(n_line), 2*np.ones(n_line)])

# Random orthogonal matrix for embedding 3D -> 100D
# Use QR decomposition to get orthonormal columns
random_matrix = np.random.randn(embed_dim, 3)
Q, _ = np.linalg.qr(random_matrix)
embedding_matrix = Q[:, :3].T  # (3, 100)

# Embed into 100D
X_hd = X_3d @ embedding_matrix

X_hd.shape, labels_hd.shape
#+END_SRC

#+RESULTS:
| 12000 | 5000 |
| 12000 |      |

#+BEGIN_SRC python :session ssc.org  :exports both :async
start = time.time()
ssc_hd = SparseSubspaceClusteringOMP(3, k=25, batch_size=64, use_omp=True).fit(X_hd)
elapsed = time.time() - start
#+END_SRC

#+RESULTS:
: None

#+BEGIN_SRC python :session ssc.org  :exports both :async
f"Fit time: {int(elapsed // 60)}m {int(elapsed) % 60}s"
#+END_SRC

#+RESULTS:
: Fit time: 14m 11s

#+BEGIN_SRC python :session ssc.org  :exports both :async
pd.Series(ssc_hd.labels_).value_counts()
#+END_SRC

#+RESULTS:
: 0    10000
: 2     1000
: 1     1000
: Name: count, dtype: int64

