#+title: SSC

* Testing SSC implementation 
:PROPERTIES:
:CREATED:  <2025-12-29 Mon> [18:54]
:END:

Subspace clustering is an unsupervised method to identify clusters in high-dimensional data by grouping examples into subspaces.

This is achieved by finding a matrix $C$ such that $CX = X, diag(C) = 0$. Intuitively this /self-expressiveness/ - the fact that each example can be expressed as linear combination of other examples is a way to capture the fact that these examples do in fact share subspaces.

Sparse subspace clustering tries to additionally minimize $\|C\|_0$, the number of nonzero coefficients.

#+BEGIN_SRC python :session ssc.org  :exports both
import time
import pandas as pd
import numpy as np
from sae_research.sparse_subspace_clustering import SparseSubspaceClusteringOMP
#+END_SRC

#+RESULTS:
: None

** Synthetic data 
:PROPERTIES:
:CREATED:  <2025-12-29 Mon> [18:56]
:END:

Dataset setup: we will use the example from "Generalized Principal Component Analysis"

Consider 3 subspaces in R^3: the xy plane and lines z=x, z=-x

We'll sample 10 points from each of these subspaces

#+BEGIN_SRC python :session ssc.org  :exports both
np.random.seed(42)
n_points_per_subspace = 10

# Subspace 1: xy plane (z=0)
xy_plane = np.column_stack([
    np.random.randn(n_points_per_subspace),
    np.random.randn(n_points_per_subspace),
    np.zeros(n_points_per_subspace)
])

# Subspace 2: line z=x (y=0)
t2 = np.random.randn(n_points_per_subspace)
line_zx = np.column_stack([t2, np.zeros(n_points_per_subspace), t2])

# Subspace 3: line z=-x (y=0)
t3 = np.random.randn(n_points_per_subspace)
line_znx = np.column_stack([t3, np.zeros(n_points_per_subspace), -t3])

# Stack all points
X = np.vstack([xy_plane, line_zx, line_znx])
# True labels
labels = np.repeat([0, 1, 2], n_points_per_subspace)

X
#+END_SRC

#+RESULTS:
#+begin_example
array([[ 0.49671415, -0.46341769,  0.        ],
       [-0.1382643 , -0.46572975,  0.        ],
       [ 0.64768854,  0.24196227,  0.        ],
       [ 1.52302986, -1.91328024,  0.        ],
       [-0.23415337, -1.72491783,  0.        ],
       [-0.23413696, -0.56228753,  0.        ],
       [ 1.57921282, -1.01283112,  0.        ],
       [ 0.76743473,  0.31424733,  0.        ],
       [-0.46947439, -0.90802408,  0.        ],
       [ 0.54256004, -1.4123037 ,  0.        ],
       [ 1.46564877,  0.        ,  1.46564877],
       [-0.2257763 ,  0.        , -0.2257763 ],
       [ 0.0675282 ,  0.        ,  0.0675282 ],
       [-1.42474819,  0.        , -1.42474819],
       [-0.54438272,  0.        , -0.54438272],
       [ 0.11092259,  0.        ,  0.11092259],
       [-1.15099358,  0.        , -1.15099358],
       [ 0.37569802,  0.        ,  0.37569802],
       [-0.60063869,  0.        , -0.60063869],
       [-0.29169375,  0.        , -0.29169375],
       [-0.60170661,  0.        ,  0.60170661],
       [ 1.85227818,  0.        , -1.85227818],
       [-0.01349722,  0.        ,  0.01349722],
       [-1.05771093,  0.        ,  1.05771093],
       [ 0.82254491,  0.        , -0.82254491],
       [-1.22084365,  0.        ,  1.22084365],
       [ 0.2088636 ,  0.        , -0.2088636 ],
       [-1.95967012,  0.        ,  1.95967012],
       [-1.32818605,  0.        ,  1.32818605],
       [ 0.19686124,  0.        , -0.19686124]])
#+end_example

Let's see if SSC was able to correctly identify subspaces:

#+BEGIN_SRC python :session ssc.org  :exports both
start = time.time()
ssc = SparseSubspaceClusteringOMP(3, k=3).fit(X)
f"Fit time: {time.time() - start:.3f}s"
#+END_SRC

#+RESULTS:
: Fit time: 0.029s

#+BEGIN_SRC python :session ssc.org  :exports both
ssc.labels_
#+END_SRC

#+RESULTS:
: array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0,
:        0, 0, 0, 0, 0, 0, 0, 0], dtype=int32)

Since the ordering of labels doesn't mean anything, it means that SSC works.

Now let's check the self-expressiveness

$CX = X$

We'll check $CX - X$

#+BEGIN_SRC python :session ssc.org  :exports both
(ssc._coefs.toarray() @ X) - X
#+END_SRC

#+RESULTS:
#+begin_example
array([[ 1.09583348, -1.32643187,  0.        ],
       [ 0.07855707, -0.18217135,  0.        ],
       [ 0.13669636,  0.02835514,  0.        ],
       [-1.1608021 ,  1.36631686,  0.        ],
       [ 0.34959774,  1.15422614,  0.        ],
       [-0.1022171 , -0.42160877,  0.        ],
       [-0.93319286,  0.66495604,  0.        ],
       [-0.13672333, -0.02836077,  0.        ],
       [ 0.10108463,  0.41690397,  0.        ],
       [ 0.73397914, -0.62198458,  0.        ],
       [-1.23987247,  0.        , -1.23987247],
       [-1.23987247,  0.        , -1.23987247],
       [ 1.39812056,  0.        ,  1.39812056],
       [-0.04090058,  0.        , -0.04090058],
       [-0.92126604,  0.        , -0.92126604],
       [ 1.35472618,  0.        ,  1.35472618],
       [-0.31465519,  0.        , -0.31465519],
       [ 1.08995075,  0.        ,  1.08995075],
       [-0.86501008,  0.        , -0.86501008],
       [-1.17395502,  0.        , -1.17395502],
       [ 0.58820939,  0.        , -0.58820939],
       [-1.83878096,  0.        ,  1.83878096],
       [-1.20734643,  0.        ,  1.20734643],
       [ 1.0442137 ,  0.        , -1.0442137 ],
       [-0.80904769,  0.        ,  0.80904769],
       [ 1.20734643,  0.        , -1.20734643],
       [-0.19536637,  0.        ,  0.19536637],
       [ 1.9461729 ,  0.        , -1.9461729 ],
       [ 1.31468882,  0.        , -1.31468882],
       [-0.18336401,  0.        ,  0.18336401]])
#+end_example

That doesn't work but it's because *OMP normalizes columns*

#+BEGIN_SRC python :session ssc.org  :exports both
X_normed = X / np.linalg.norm(X, axis=1)[:, np.newaxis]

normed_diff = (ssc._coefs.toarray() @ X_normed) - X_normed
normed_diff
#+END_SRC

#+RESULTS:
#+begin_example
array([[ 8.40946972e-04,  3.23063818e-04,  0.00000000e+00],
       [-1.52685355e-03,  7.89426903e-04,  0.00000000e+00],
       [ 2.20497117e-06, -5.90230264e-06,  0.00000000e+00],
       [ 1.99586291e-03,  3.11196232e-03,  0.00000000e+00],
       [-3.51335028e-03,  1.81650246e-03,  0.00000000e+00],
       [-1.23788516e-03,  5.15456325e-04,  0.00000000e+00],
       [-2.43341726e-03, -3.79419988e-03,  0.00000000e+00],
       [-2.38759096e-06,  5.83082187e-06,  0.00000000e+00],
       [ 1.19112840e-03, -6.15847407e-04,  0.00000000e+00],
       [-1.71834439e-03, -6.60130683e-04,  0.00000000e+00],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00],
       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00]])
#+end_example

We can check that all the elements are close to zero:

#+BEGIN_SRC python :session ssc.org  :exports both
np.all(np.isclose(normed_diff, 0))
#+END_SRC

#+RESULTS:
: False

*** More dimensions... 
:PROPERTIES:
:CREATED:  <2025-12-29 Mon> [19:24]
:END:

We'll use the same example but with a twist: bump number of points to 300 for 1st subspace, 50 for 2nd and 3rd and embed all this data into a larger 100d space so it's harder computationally

To do this we will create data like before and use a random orthogonal matrix to rotate this stuff into 100d 

#+BEGIN_SRC python :session ssc.org  :exports both
np.random.seed(42)
n_plane = 10000 
n_line = 1000
embed_dim = 5000 

# Subspace 1: xy plane (z=0) - 300 points
xy_plane_hd = np.column_stack([
    np.random.randn(n_plane),
    np.random.randn(n_plane),
    np.zeros(n_plane)
])

# Subspace 2: line z=x (y=0) - 50 points
t2 = np.random.randn(n_line)
line_zx_hd = np.column_stack([t2, np.zeros(n_line), t2])

# Subspace 3: line z=-x (y=0) - 50 points
t3 = np.random.randn(n_line)
line_znx_hd = np.column_stack([t3, np.zeros(n_line), -t3])

# Stack all points in 3D
X_3d = np.vstack([xy_plane_hd, line_zx_hd, line_znx_hd])
labels_hd = np.concatenate([np.zeros(n_plane), np.ones(n_line), 2*np.ones(n_line)])

# Random orthogonal matrix for embedding 3D -> 100D
# Use QR decomposition to get orthonormal columns
random_matrix = np.random.randn(embed_dim, 3)
Q, _ = np.linalg.qr(random_matrix)
embedding_matrix = Q[:, :3].T  # (3, 100)

# Embed into 100D
X_hd = X_3d @ embedding_matrix

X_hd.shape, labels_hd.shape
#+END_SRC

#+RESULTS:
| 12000 | 5000 |
| 12000 |      |

#+BEGIN_SRC python :session ssc.org  :exports both :async
start = time.time()
ssc_hd = SparseSubspaceClusteringOMP(3, k=25, batch_size=64).fit(X_hd)
f"Fit time: {time.time() - start:.3f}s"
#+END_SRC

#+RESULTS:
: /var/folders/t7/9zd9yqf17_zbfq2hwm8tv9ym0000gn/T/babel-dMQfgQ/python-CzqTA3

#+BEGIN_SRC python :session ssc.org  :exports both
pd.Series(ssc_hd.labels_).value_counts()
#+END_SRC

#+RESULTS:
: 0    10000
: 2     1000
: 1     1000
: Name: count, dtype: int64
