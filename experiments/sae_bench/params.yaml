# SAE Bench Pipeline Configuration

# Random seed for reproducibility
random_seed: 42

# Model configuration
model:
  name: "gemma-2-2b"
  llm_batch_size: 128
  torch_dtype: "bfloat16"
  device: "cuda"  # or "cpu"

# Baseline SAE configuration
# This loads ONE specific SAE directly from HuggingFace
# Used as the base for creating custom variants (IHTP, MPSAE)
baseline_sae:
  repo_id: "canrager/lm_sae"  # HuggingFace repository
  filename: "gemma-2-2b_sweep_standard_ctx128_ef2_0824/resid_post_layer_19/trainer_5/ae.pt"  # Exact file path in repo
  hook_layer: 19
  training_tokens: 200_000_000

# SAE variant configurations
sae_variants:
  # Instance Hard Thresholding Pursuit SAE
  ihtp:
    enabled: true
    k_values: [5, 10]  # Top-k values for IHTP
  # Matching Pursuit SAE
  mpsae:
    enabled: true
    s_values: [50, 100]  # Sparsity levels for MPSAE

# Baseline comparison SAEs from SAE Bench
comparison_saes:
  enabled: true
  sae_regex_pattern: "(gemma-2-2b_sweep_topk_ctx128_ef2_0824).*"
  sae_block_pattern: ".*blocks\\.([19])\\.hook_resid_post__trainer_(5)$"

# Evaluation types to run
eval_types:
  core: true
  sparse_probing: true
  absorption: false  # Not recommended for models < 2B parameters
  scr: true
  tpp: true
  autointerp: false  # Must be run as script, not in notebook
  unlearning: false  # Requires instruct-tuned model >= 2B params

# Core evaluation settings
core_eval:
  n_eval_reconstruction_batches: 2000
  n_eval_sparsity_variance_batches: 2000
  eval_batch_size_prompts: 8
  compute_featurewise_density_statistics: true
  compute_featurewise_weight_based_metrics: true
  exclude_special_tokens_from_reconstruction: true
  dataset: "Skylion007/openwebtext"
  context_size: 128

# Sparse probing evaluation settings
sparse_probing:
  dataset_names:
    - "LabHC/bias_in_bios_class_set1"
  # Additional datasets can be added here:
  # - "dataset_name_2"
  # - "dataset_name_3"
  sae_batch_size: 256

# Activation caching settings
# Set to true if evaluating multiple SAEs on the same layer
# Requires at least 100GB of disk space
activation_caching:
  save_activations: false

# Visualization settings
visualization:
  # Top-k value for accuracy metrics in plots
  k: 1

  # Marker shapes for different SAE architectures
  trainer_markers:
    standard: "o"
    jumprelu: "X"
    topk: "^"
    p_anneal: "*"
    gated: "d"
    vanilla: "s"
    ihtp: "p"  # pentagon for IHTP
    mpsae: "D"  # diamond for MPSAE

  # Colors for different training configurations
  trainer_colors:
    standard: "blue"
    jumprelu: "orange"
    topk: "green"
    p_anneal: "red"
    gated: "purple"
    vanilla: "black"
    ihtp: "cyan"
    mpsae: "magenta"
