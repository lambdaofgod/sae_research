# -*- coding: utf-8 -*-
"""SAE Lens basic_loading_and_analysing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1assS27KybAFvJ4gdKtntybTZVgf38l3U

# Loading and Analysing Pre-Trained Sparse Autoencoders

## Imports & Installs
"""

# Commented out IPython magic to ensure Python compatibility.
try:
    import google.colab  # type: ignore
    from google.colab import output

    COLAB = True
#     %pip install sae-lens transformer-lens sae-dashboard
except:
    COLAB = False
    from IPython import get_ipython  # type: ignore

    ipython = get_ipython()
    assert ipython is not None
    ipython.run_line_magic("load_ext", "autoreload")
    ipython.run_line_magic("autoreload", "2")

# Standard imports
import os
import torch
from tqdm import tqdm
import plotly.express as px
from dotenv import load_dotenv

# Imports for displaying vis in Colab / notebook
import webbrowser
import http.server
import socketserver
import threading
import trackio


# +
def get_plot_as_img():
    fig = plt.gcf()
    fig.canvas.draw()
    img = Image.frombytes('RGBA', fig.canvas.get_width_height(),
                        fig.canvas.tostring_argb())
    plt.close()
    a, r, g, b = img.split()
    img = Image.merge('RGB', (r, g, b))
    return img

def get_hf_token():
    """
    Gets HuggingFace token from either Google Colab userdata or .env file.
    Sets the token in os.environ["HF_TOKEN"].
    """
    if COLAB:
        from google.colab import userdata
        os.environ["HF_TOKEN"] = userdata.get("HF_TOKEN")
    else:
        load_dotenv()
        token = os.getenv("HF_TOKEN")
        os.environ["HF_TOKEN"] = token



class Tracker:

    #def __init__(self, project, name, config, space_id=None):
    #    
    
    def log_plot_to_trackio(self, plot_title, plot_metadata):
        img = get_plot_as_img()
        images = trackio.Image(img, caption=plot_title)
        
        trackio.log({"plots": images})

    def log_dataframe_to_trackio(self, name, df):
        trackio.log({
              name: trackio.Table(dataframe=df)
          })

class SAELensTracker(Tracker):

    def __init__(self, project: str, sae_config: dict, space_id=None):
        name = sae_config["release"] + "+" + sae_config["sae_id"]
        trackio.init(project, name, config=sae_config, space_id=space_id)#f"{project}/{name.replace('/', '-')}")
        #super().__init__(project, name, sae_config, space_id)


# +
## Setup

# +
sae_config = dict(
    
    release="gemma-scope-2b-pt-res",  # see other options in sae_lens/pretrained_saes.yaml
    sae_id="layer_19/width_16k/average_l0_279",
)

tracker = SAELensTracker("sae_experiments", sae_config)#, space_id="lambdaofgod/sae_experiments")
# -

PORT = 8000

torch.set_grad_enabled(False)

"""## Set Up

"""

# For the most part I'll try to import functions and classes near where they are used
# to make it clear where they come from.

if torch.backends.mps.is_available():
    device = "mps"
else:
    device = "cuda" if torch.cuda.is_available() else "cpu"

print(f"Device: {device}")


def display_vis_inline(filename: str, height: int = 850):
    """
    Displays the HTML files in Colab. Uses global `PORT` variable defined in prev cell, so that each
    vis has a unique port without having to define a port within the function.
    """
    if not (COLAB):
        webbrowser.open(filename)

    else:
        global PORT

        def serve(directory):
            os.chdir(directory)

            # Create a handler for serving files
            handler = http.server.SimpleHTTPRequestHandler

            # Create a socket server with the handler
            with socketserver.TCPServer(("", PORT), handler) as httpd:
                print(f"Serving files from {directory} on port {PORT}")
                httpd.serve_forever()

        thread = threading.Thread(target=serve, args=("/content",))
        thread.start()

        output.serve_kernel_port_as_iframe(
            PORT, path=f"/{filename}", height=height, cache_in_notebook=True
        )

        PORT += 1


"""# Loading a pretrained Sparse Autoencoder

Below we load a Transformerlens model, a pretrained SAE and a dataset from huggingface.

"""

get_hf_token()

from datasets import load_dataset
from transformer_lens import HookedTransformer
from sae_lens import SAE
from sae_research.instance_sae import load_from_sae_lens

model = HookedTransformer.from_pretrained("gemma-2-2b", device=device, dtype=torch.float16)

# the cfg dict is returned alongside the SAE since it may contain useful information for analysing the SAE (eg: instantiating an activation store)
# Note that this is not the same as the SAEs config dict, rather it is whatever was in the HF repo, from which we can extract the SAE config dict
# We also return the feature sparsities which are stored in HF for convenience.
sae = SAE.from_pretrained(
    **sae_config,
    device=device,
)



import numpy as np
import sklearn

modules = sae.mod_dict

M_Decoder = sae.W_dec.cpu().numpy()
M_Decoder.shape
Gram_Decoder = sklearn.metrics.pairwise.cosine_similarity(M_Decoder)
np.fill_diagonal(Gram_Decoder, np.zeros(Gram_Decoder.shape[0]))
Gram_Decoder



Gram_Decoder.shape

Gram_Decoder.max()

Gram_Decoder.shape[0] ** 2

len(np.where(Gram_Decoder > 0.1)[0])

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from PIL import Image


upper_triangular = Gram_Decoder[np.triu_indices(Gram_Decoder.shape[0], k=1)]
coherences = pd.Series(upper_triangular.reshape(-1))
coherences.describe()

# +
#tracker.log_dataframe_to_trackio("W_d coherences", pd.DataFrame(coherences))# pd.DataFrame(Gram_Decoder))

# +
#tracker.log_dataframe_to_trackio("W_d coherences summary", pd.DataFrame(coherences.describe()))# pd.DataFrame(Gram_Decoder))
# -

coherences.quantile(0.99)

coherences.plot.hist()

# +
#sns.histplot(coherences)
#tracker.log_plot_to_trackio("W_d coherence histogram", {})
# -

coherences.quantile(0.90)

# +
#sns.heatmap(Gram_Decoder)
#tracker.log_plot_to_trackio("W_d (normalized) Gram matrix heatmap", {})# pd.DataFrame(Gram_Decoder))
# -

M_Decoder.shape


# +

def mutual_coherence_lower_bound(N, k):
    """
    theoretical lower bound on mutual coherence
    """
    return np.sqrt((N-k) / (k * (N - 1)))

K = 2304


plt.plot(np.arange(K+1, 256000),mutual_coherence_lower_bound(np.arange(K+1, 256000), K))
get_plot_as_img()
# -
mutual_coherence_lower_bound(16000, 2304)


# +
#trackio.finish()

# +
#trackio.Image()
# -

from transformer_lens.utils import tokenize_and_concatenate

dataset = load_dataset(
    path="NeelNanda/pile-10k",
    split="train",
    streaming=False,
)

token_dataset = tokenize_and_concatenate(
    dataset=dataset,  # type: ignore
    tokenizer=model.tokenizer,  # type: ignore
    streaming=True,
    max_length=128,#sae.cfg.metadata.context_size,
    add_bos_token=sae.cfg.metadata.prepend_bos,
)


"""## Basic Analysis

Let's check some basic stats on this SAE in order to see how some basic functionality in the codebase works.

We'll calculate:

- L0 (the number of features that fire per activation)
- The cross entropy loss when the output of the SAE is used in place of the activations

"""


"""### L0 Test and Reconstruction Test

"""

sae.eval()  # prevents error if we're expecting a dead neuron mask for who grads

# +
batch_tokens = token_dataset[:32]["tokens"]

batch_tokens.shape

# +
from torch.profiler import profile, ProfilerActivity, record_function

activities = [ProfilerActivity.CPU]
if torch.cuda.is_available():
    activities += [ProfilerActivity.CUDA]

with profile(activities=activities, record_shapes=True, profile_memory=True) as prof:
    with record_function("model_inference"):
        with torch.no_grad():
            # activation store can give us tokens.
            batch_tokens = token_dataset[:8]["tokens"]
            _, cache = model.run_with_cache(batch_tokens, prepend_bos=True)

# -

print(
    prof.key_averages(group_by_input_shape=True).table(
        sort_by="cpu_time_total", row_limit=10
    )
)

# %%time
with torch.no_grad():
    # activation store can give us tokens.
    batch_tokens = token_dataset[:4]["tokens"]
    _, cache = model.run_with_cache(batch_tokens, prepend_bos=True)

    # Use the SAE
    feature_acts = sae.encode(cache[sae.cfg.metadata.hook_name])
    sae_out = sae.decode(feature_acts).half()

    # save some room
    del cache

    # ignore the bos token, get the number of features that activated in each token, averaged accross batch and position
    l0 = (feature_acts[:, 1:] > 0).float().sum(-1).detach()
    print("average l0", l0.mean().item())
    px.histogram(l0.flatten().cpu().numpy()).show()

"""Note that while the mean L0 is 64, it varies with the specific activation.

To estimate reconstruction performance, we calculate the CE loss of the model with and without the SAE being used in place of the activations. This will vary depending on the tokens.

"""

from transformer_lens import utils
from functools import partial

sae.W_dec.shape


# +
## Tutaj coś jest nie halo, za dużo jest wyzerowane

# +
def threshold_topk(acts, topk):
    acts_topk = torch.topk(acts.abs(), topk, dim=-1)
    acts_topk = torch.zeros_like(acts).scatter(
        -1, acts_topk.indices, torch.gather(acts, -1, acts_topk.indices)
    )
    return acts_topk


def instance_threshold_topk(acts, topk):
    acts_flat = acts.reshape(acts.shape[0], -1)
    abs_acts_topk = torch.topk(acts_flat.abs(), topk * acts.shape[1], dim=-1)
    acts_out = torch.zeros_like(acts_flat).scatter(
        -1, abs_acts_topk.indices, torch.gather(acts_flat, -1, abs_acts_topk.indices)
    )
    return acts_out.reshape(acts.shape)


def iterative_hard_thresholding(A, y, b, topk):
    x_next = (y-b) @ A.T
    return threshold_topk(x_next, topk)


def instance_iterative_hard_thresholding(A, y, b, topk):
    x_next = (y-b) @ A.T
    return instance_threshold_topk(x_next, topk)


# -

# Naive code with thresholding + least squares on support

# +
def sparse_lstq(A, Y, support):
    """
    find X
    A[:,support]X = Y
    """
    A_in = A[support].float().T
    Y_in = Y.float().T
    coefficients = torch.linalg.lstsq(A_in, Y_in).solution.T
    x = torch.zeros(Y.shape[0], A.shape[0]).to(Y.device)
    x[:,support] = coefficients
    return x

def instance_thresholding_support(W_dec, y, topk):
    """
    get support like in thresholding algorithm and then fit linear regression on this support
    """
    return torch.topk((y @ W_dec.T).abs(), topk).indices[:,-1].unique()


# -

ihtp_sae = load_from_sae_lens(
    model_name="gemma-2-2b",
    release=sae_config["release"],
    sae_id=sae_config["sae_id"],
    k=10,  # per-position top-k
    hook_layer=19,
    device=device,
).half()


# +
def count_l0_per_token(x):
    l0 = (x[:, 1:] > 0).float().sum(-1).detach()
    return l0


def count_l0_per_instance(x):
    l0 = (x[:, 1:] > 0).float().max(1).values.sum(-1).detach()
    return l0

def print_l0(x):
    print(f"l0 per token: {count_l0_per_token(x).mean().item()}")
    print(f"l0 per instance: {count_l0_per_instance(x).mean().item()}")


# +
W_dec = sae.W_dec.half()
b_dec = sae.b_dec.half()
b_enc = sae.b_enc.half()


# next we want to do a reconstruction test.
EPS = 1e-8

def reconstr_hook(activation, hook, sae_out):
    print("sae nonzero")
    print_l0(feature_acts)
    return sae_out


def hard_thresholding_hook(activation, hook):
    x =  iterative_hard_thresholding(W_dec, activation, b_dec, 100)
    print("iht nonzero")
    print_l0(x)
    return x @ W_dec + b_dec


def instance_hard_thresholding_hook(activation, hook):
    x =  instance_iterative_hard_thresholding(W_dec, activation, b_dec, 100)
    print("instance iht nonzero")
    print_l0(x)
    return x @ W_dec + b_dec


def ihtp_hook(activation, hook):
    x = ihtp_sae.encode(activation)
    print("ihtp nonzero")
    print_l0(x)
    return ihtp_sae.decode(x)


# -
def zero_abl_hook(activation, hook):
    return torch.zeros_like(activation)


print("###")
print("Orig", model(batch_tokens, return_type="loss").item())
print("###")
print(
    "reconstr_sae",
    model.run_with_hooks(
        batch_tokens,
        fwd_hooks=[
            (
                sae.cfg.metadata.hook_name,
                partial(reconstr_hook, sae_out=sae_out),
            )
        ],
        return_type="loss",
    ).item(),
)
print("###")
print(
    "reconstr_iht",
    model.run_with_hooks(
        batch_tokens,
        return_type="loss",
        fwd_hooks=[(sae.cfg.metadata.hook_name, hard_thresholding_hook)],
    ).item(),
)
print("###")
print(
    "reconstr_instance_iht",
    model.run_with_hooks(
        batch_tokens,
        return_type="loss",
        fwd_hooks=[(sae.cfg.metadata.hook_name, instance_hard_thresholding_hook)],
    ).item(),
)
print("###")
print(
    "reconstr_ihtp",
    model.run_with_hooks(
        batch_tokens,
        return_type="loss",
        fwd_hooks=[(sae.cfg.metadata.hook_name, ihtp_hook)],
    ).item(),
)
print("###")
print(
    "Zero",
    model.run_with_hooks(
        batch_tokens,
        return_type="loss",
        fwd_hooks=[(sae.cfg.metadata.hook_name, zero_abl_hook)],
    ).item(),
)

"""## Specific Capability Test

Validating model performance on specific tasks when using the reconstructed activation is quite important when studying specific tasks.

"""

example_prompt = "When John and Mary went to the shops, John gave the bag to"
example_answer = " Mary"
utils.test_prompt(example_prompt, example_answer, model, prepend_bos=True)

logits, cache = model.run_with_cache(example_prompt, prepend_bos=True)
tokens = model.to_tokens(example_prompt)
sae_out = sae(cache[sae.cfg.metadata.hook_name])


def reconstr_hook(activations, hook, sae_out):
    return sae_out


def zero_abl_hook(mlp_out, hook):
    return torch.zeros_like(mlp_out) 


hook_name = sae.cfg.metadata.hook_name

print("Orig", model(tokens, return_type="loss").item())
print(
    "reconstr",
    model.run_with_hooks(
        tokens,
        fwd_hooks=[
            (
                hook_name,
                partial(reconstr_hook, sae_out=sae_out),
            )
        ],
        return_type="loss",
    ).item(),
)
print(
    "Zero",
    model.run_with_hooks(
        tokens,
        return_type="loss",
        fwd_hooks=[(hook_name, zero_abl_hook)],
    ).item(),
)


with model.hooks(
    fwd_hooks=[
        (
            hook_name,
            partial(reconstr_hook, sae_out=sae_out),
        )
    ]
):
    utils.test_prompt(example_prompt, example_answer, model, prepend_bos=True)

"""# Generating Feature Interfaces

Feature dashboards are an important part of SAE Evaluation. They work by:

- 1. Collecting feature activations over a larger number of examples.
- 2. Aggregating feature specific statistics (such as max activating examples).
- 3. Representing that information in a standardized way

For our feature visualizations, we will use a separate library called SAEDashboard.

"""

# Make sure to install sae-dashboard if not running in colab
# pip install sae-dashboard
# Note: this cell may not work until sae-dashboard is updated to work with the latest version of sae-lens

test_feature_idx_gpt = list(range(10)) + [14057]

from sae_dashboard.sae_vis_data import SaeVisConfig
from sae_dashboard.sae_vis_runner import SaeVisRunner


feature_vis_config_gpt = SaeVisConfig(
    hook_point=hook_name,
    features=test_feature_idx_gpt,
    minibatch_size_features=64,
    minibatch_size_tokens=256,
    verbose=True,
    device=device,
)

visualization_data_gpt = SaeVisRunner(feature_vis_config_gpt).run(
    encoder=sae,  # type: ignore
    model=model,
    tokens=token_dataset[:10000]["tokens"],  # type: ignore
)
# SaeVisData.create(
#     encoder=sae,
#     model=model, # type: ignore
#     tokens=token_dataset[:10000]["tokens"],  # type: ignore
#     cfg=feature_vis_config_gpt,
# )

from sae_dashboard.data_writing_fns import save_feature_centric_vis

filename = f"demo_feature_dashboards.html"
save_feature_centric_vis(sae_vis_data=visualization_data_gpt, filename=filename)

"""Now, since generating feature dashboards can be done once per sparse autoencoder, for pre-trained SAEs in the public domain, everyone can use the same dashboards. Neuronpedia hosts dashboards which we can load via the integration.

"""

from sae_lens.analysis.neuronpedia_integration import get_neuronpedia_quick_list

# this function should open
neuronpedia_quick_list = get_neuronpedia_quick_list(sae, test_feature_idx_gpt)

if COLAB:
    # If you're on colab, click the link below
    print(neuronpedia_quick_list)
